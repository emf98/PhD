{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f4984-2b54-4886-a5a2-ee7805df467d",
   "metadata": {},
   "source": [
    "#### Code to download and separate geopotential height data.\n",
    "\n",
    "Updated: March 25, 2025\n",
    "\n",
    "... lol. I am now taking Zheng's data. \n",
    "2 degree resolution for this data, thankfully. No regridding needed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d2e30d-e0e9-4474-9392-a58069d8437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements... I think these are all of the relevant ones to what I am doing here. \n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "import pickle\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e0866c-c38f-4c98-adf0-2aaef0f2c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "##indices\n",
    "infile = open(\"era5_latindex.p\", 'rb') \n",
    "var_lats = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"era5_lonindex.p\", 'rb') \n",
    "var_lons = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"era5_timeindex.p\", 'rb') \n",
    "time = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f606856c-37d8-4dfd-8a85-44fc23be59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#year = [i for i in range(1991,2022)] #indicate years for data, needed for opening files\n",
    "year = [i for i in range(1959,2021)] \n",
    "#index = [i for i in range(1,43)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc47aa7e-9541-4321-acfe-58ce19ee9224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6257d-2deb-42b1-b8f5-b6f01d89046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check levels of V datasets \n",
    "vfile = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_1960.nc\")\n",
    "vfile['var132']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d03ac-aeb8-401f-b24e-2f4ddab93831",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check levels of T datasets \n",
    "tfile = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_1959.nc\")\n",
    "tfile['var130']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623f1ed-638a-45fd-8fc7-cb4ce2446fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##check levels of GPH datasets \n",
    "gfile = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_1959.nc\")\n",
    "gfile['var129']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460f761-6bf9-4c85-bf38-e5c893c3a597",
   "metadata": {},
   "source": [
    "#### Now I am able to see that these all possess the same number of levels so ... \n",
    "\n",
    "For V ant T, I will need to reshape for (len(year), 152, 37, 13, 145). \n",
    "\n",
    "For GPH, this will be (len(year), 152, 37, 13, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36096443-f0e4-4ed5-b99e-213e402a1f92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(np.arange(75,42.5,-2.5)))\n",
    "print(len(np.arange(90,58.5,-2.5)))\n",
    "print(len(np.arange(0, 360.5, 2.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d1e58-2f64-4a1c-99ac-5929983fe978",
   "metadata": {},
   "source": [
    "#### SAVE V WIND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1d414e-18cb-455d-9c2d-f56680609322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays for the years of data\n",
    "value = np.empty((len(year),183,37,15,180))\n",
    "value[:] = np.nan"
   ]
  },
  {
   "cell_type": "raw",
   "id": "776a6a75-530e-43f1-bdd8-d0fad75f3640",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_1959.nc\")\n",
    "g_files1 = gfile1[\"var132\"] \n",
    "g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "values1 =  g_data1.time.values\n",
    "len(values1[273:])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95f1faa3-09fb-4d12-8f55-a94f346d4d5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "g_files1 = gfile1[\"var132\"] \n",
    "g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "values1 =  g_data1.values\n",
    "        \n",
    "#file 2 for end\n",
    "gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "g_files2 = gfile2[\"var132\"] #next line will reduce this to the mean \n",
    "g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "values2 =  g_data2.values\n",
    "\n",
    "\n",
    "#combine into np array\n",
    "print(values1[273:,:,:,:].shape) ## (92,)\n",
    "print(values2[:91,:,:,:].shape) ## (91,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e74cd92-3532-4dda-8723-c28735acdb54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "##V WIND FILE\n",
    "#this is my attempt at looping through and combining the files lol WITH REGRID\n",
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    ##1959:\n",
    "    if year[i] == 1959 or year[i] == 1963 or year[i] == 1967 or year[i] == 1971 or year[i] == 1975 or year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        \n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var132\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var132\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] =  values1[273:,:,:,:]\n",
    "        value[i,92:,:,:,:] =  values2[:91,:,:,:] \n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    ###1959\n",
    "    if  year[i] == 1960 or  year[i] == 1964 or  year[i] == 1968 or  year[i] == 1972 or  year[i] == 1976 or year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var132\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var132\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[274:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue\n",
    "        \n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var132\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/V/era5_an_vwind_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var132\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[273:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53c08ba-a2de-40ba-9742-14ca42c2bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.44225693,  -5.8478241 ,  -9.44345093,  -7.1171608 ,\n",
       "        -5.50409698,  -2.4201355 ,  -3.25915527,   9.42077637,\n",
       "        -1.27486801,  -9.86222839, -10.04367447,          nan,\n",
       "       -21.36532593, -24.51795959, -35.84249115, -38.18339157,\n",
       "       -35.0646019 , -36.95631027, -33.78686142, -35.54520416,\n",
       "       -38.60417557, -36.27721024, -27.48869324, -27.71282196,\n",
       "       -22.32458496, -11.1106987 ,  -8.59675598,  -0.6574707 ,\n",
       "        -1.97827148,  -3.20763397,  -2.5712204 ,  -1.25096512,\n",
       "        -2.40694046,  -1.65813446,  -2.74965286,   1.41397476,\n",
       "         6.6808548 ,   5.82423019,   3.4754982 ,   0.96657562,\n",
       "        -3.7050209 ,  -4.05934143,  -0.20933151])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1,140:,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4062df68-7e24-4fbb-a3ba-86d1229df813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(value, open(\"1959_v_2deg.p\", 'wb')) \n",
    "##gph will need to be divided by 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800d4ec-8961-4a16-a484-d2c3d7d0fb62",
   "metadata": {},
   "source": [
    "#### SAVE TEMPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3358b786-af9b-4bc8-b531-3bcebb1412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays for the years of data\n",
    "value = np.empty((len(year),183,37,15,180))\n",
    "value[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe389ad2-bce5-47fa-a4eb-dae2836cb6cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "##T FILE\n",
    "#this is my attempt at looping through and combining the files lol WITH REGRID\n",
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    ##1959:\n",
    "    if year[i] == 1959 or year[i] == 1963 or year[i] == 1967 or year[i] == 1971 or year[i] == 1975 or year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        \n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var130\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var130\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] =  values1[273:,:,:,:]\n",
    "        value[i,92:,:,:,:] =  values2[:91,:,:,:] \n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    ###1959\n",
    "    if  year[i] == 1960 or  year[i] == 1964 or  year[i] == 1968 or  year[i] == 1972 or  year[i] == 1976 or year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var130\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var130\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[274:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue\n",
    "        \n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var130\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(75,45))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/T/era5_an_temp_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var130\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(75,45))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[273:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d4790d-7e74-4982-8632-b5531b79d072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230.78050232, 234.60331726, 228.87197876, 230.91928101,\n",
       "       234.49681091, 242.18508911, 245.98617554, 250.28503418,\n",
       "       252.72102356, 250.01097107, 247.71607971, 246.00975037,\n",
       "       242.74551392, 238.84689331, 235.16783142, 238.05815125,\n",
       "       241.76373291, 242.79318237, 240.17477417, 241.87438965,\n",
       "       248.76979065, 245.07762146, 237.37896729, 240.74385071,\n",
       "       245.07728577, 250.71737671, 257.15518188, 256.16665649,\n",
       "       266.19421387, 276.88269043, 274.04748535, 264.7565918 ,\n",
       "       247.21479797, 236.54917908, 242.70629883, 248.54107666,\n",
       "       252.37319946, 252.96290588, 255.85624695, 249.93043518])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1,90:130,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d00e695-f2c9-457e-983d-4faf920f6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(value, open(\"1959_t_2deg.p\", 'wb')) \n",
    "##gph will need to be divided by 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf8855-0fca-4ff8-86dc-8c5bce7eca8d",
   "metadata": {},
   "source": [
    "#### SAVE GPH. \n",
    "\n",
    "I need to fix this to be just N of 20. lol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324f9667-c525-427a-b24b-fb2670493d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays for the years of data\n",
    "value = np.empty((len(year),183,37,36,180))\n",
    "value[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db507e39-c410-423c-87b9-646ba5d766be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "##GPH FILE\n",
    "#this is my attempt at looping through and combining the files lol WITH REGRID\n",
    "for i in range(len(year)):\n",
    "    print(year[i])\n",
    "    #start by designating leap years because those arrays will be of a different size\n",
    "    ##1959:\n",
    "    if year[i] == 1959 or year[i] == 1963 or year[i] == 1967 or year[i] == 1971 or year[i] == 1975 or year[i] == 1979 or year[i] == 1983 or year[i] == 1987 or year[i] == 1991 or year[i] == 1995 or year[i] == 1999 or year[i] == 2003 or year[i] == 2007 or year[i] == 2011 or year[i] == 2015 or year[i] == 2019:\n",
    "        \n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var129\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(90,20))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var129\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(90,20))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] =  values1[273:,:,:,:]\n",
    "        value[i,92:,:,:,:] =  values2[:91,:,:,:] \n",
    "        continue\n",
    "        \n",
    "    #this loop is because of the weird indexing in years of leap years\n",
    "    ###1959\n",
    "    if  year[i] == 1960 or  year[i] == 1964 or  year[i] == 1968 or  year[i] == 1972 or  year[i] == 1976 or year[i] == 1980 or year[i] == 1984 or year[i] == 1988 or year[i] == 1992 or year[i] == 1996 or year[i] == 2000 or year[i] == 2004 or year[i] == 2008 or year[i] == 2012 or year[i] == 2016 or year[i] == 2020:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var129\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(90,20))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var129\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(90,20))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[274:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue\n",
    "        \n",
    "    #everything else and repeat \n",
    "    else:\n",
    "        gfile1 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i])+\".nc\")\n",
    "        g_files1 = gfile1[\"var129\"] \n",
    "        g_data1 = g_files1.loc[dict(lat=slice(90,20))]\n",
    "        values1 =  g_data1.values\n",
    "        \n",
    "        #file 2 for end\n",
    "        gfile2 = xr.open_dataset(\"/network/rit/lab/wulab/Datasets/ERA5/daily/GPH/era5_an_geopot_reg2_daily_\"+str(year[i]+1)+\".nc\")\n",
    "        g_files2 = gfile2[\"var129\"] #next line will reduce this to the mean \n",
    "        g_data2 = g_files2.loc[dict(lat=slice(90,20))]\n",
    "        values2 =  g_data2.values\n",
    "\n",
    "        #combine into np array\n",
    "        value[i,:92,:,:,:] = values1[273:,:,:,:]\n",
    "        value[i,92:151,:,:,:] = values2[:59,:,:,:]\n",
    "        value[i,152:,:,:,:] = values2[60:91,:,:,:]\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa2a6f6-75fa-4bb9-8a83-1e7c7da2cfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([429456.     , 428869.8125 , 428580.0625 , 428148.59375,\n",
       "       427682.25   , 427440.90625, 428091.3125 , 430794.3125 ,\n",
       "       430782.59375, 429921.875  , 430747.71875, 431505.8125 ,\n",
       "       431126.9375 , 431601.75   , 431821.53125, 433879.5    ,\n",
       "       444614.9375 , 451229.6875 , 448611.9375 , 450670.75   ,\n",
       "       444359.375  , 435067.375  , 431951.8125 , 434147.625  ,\n",
       "       439479.     , 444938.3125 , 447236.09375, 444559.96875,\n",
       "       439270.15625, 434734.46875, 431787.6875 , 427930.75   ,\n",
       "       426705.78125, 428961.5    , 428007.03125, 427133.375  ,\n",
       "       426347.8125 , 425272.0625 , 426040.6875 , 427604.53125])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1,90:130,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743b646c-091c-4ff2-88c7-8721e39a0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = value/9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d7608f-bafa-4dda-9d89-bdb424f22741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(value, open(\"1959_gph_2deg.p\", 'wb')) \n",
    "##gph will need to be divided by 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8904d70-122c-46a5-806e-926e19dbffb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43777.37003058, 43717.61595311, 43688.07976555, 43644.09722222,\n",
       "       43596.55963303, 43571.95782365, 43638.25815494, 43913.79332314,\n",
       "       43912.59875127, 43824.8598369 , 43909.0437054 , 43986.32135576,\n",
       "       43947.70005097, 43996.10091743, 44018.50471458, 44228.28746177,\n",
       "       45322.62359837, 45996.91004077, 45730.06498471, 45939.93374108,\n",
       "       45296.57237513, 44349.3756371 , 44031.7851682 , 44255.61926606,\n",
       "       44799.08256881, 45355.58741081, 45589.81587666, 45317.02025994,\n",
       "       44777.7937054 , 44315.44023955, 44015.05479103, 43621.89092762,\n",
       "       43497.02153415, 43726.96228338, 43629.66679409, 43540.60907238,\n",
       "       43460.53134557, 43350.87283384, 43429.22400612, 43588.63723242])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1,90:130,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e96c6a-bdb9-438b-84f7-3c896a659c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 August 2021 Environment",
   "language": "python",
   "name": "aug21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
